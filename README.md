# G1 Humanoid Factory Manipulation Environment# G1 Humanoid Factory Manipulation Environment# G1 Humanoid Factory Environment - Isaac Lab



Reinforcement learning environment for training G1 humanoid robots to perform factory manipulation tasks using NVIDIA Isaac Lab and Ray for distributed training.



## Overview[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)This project implements a reinforcement learning environment for training G1 humanoids to perform factory manipulation tasks using NVIDIA Isaac Lab and Ray for distributed training.



This project implements a complete RL pipeline with:[![Isaac Lab](https://img.shields.io/badge/Isaac%20Lab-2.1.0+-green.svg)](https://github.com/isaac-sim/IsaacLab)

- G1 humanoid robot (23 DOFs) with joint position control for arm manipulation

- 52-dimensional observation space (joint states, base velocities)[![Python 3.11+](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)## Project Structure

- Action smoothness penalty reward

- PPO algorithm via RSL-RL

- Distributed training with Ray cluster

- WANDB integration with video recordingA reinforcement learning environment for training humanoid robots (Unitree G1) to perform factory manipulation tasks using NVIDIA Isaac Lab with distributed training via Ray.```



## Project Structureg1_factory_isaac/



```## ðŸ“‹ Table of Contentsâ”œâ”€â”€ assets/                  # Robot and object asset configurations

g1_factory_isaac/

â”œâ”€â”€ assets/                 # G1 robot URDF, meshes, configsâ”œâ”€â”€ tasks/                   # Environment task configurations

â”œâ”€â”€ tasks/                  # Environment configuration

â”œâ”€â”€ mdp/                    # Reward and observation functions- [Overview](#overview)â”œâ”€â”€ mdp/                     # Reward/observation/action definitions

â”œâ”€â”€ agents/                 # PPO training config

â”œâ”€â”€ scripts/- [Project Structure](#project-structure)â”œâ”€â”€ agents/                  # RL agent configurations (PPO)

â”‚   â”œâ”€â”€ train.py           # Training script

â”‚   â”œâ”€â”€ play.py            # Policy evaluation (TODO)- [Implementation Status](#implementation-status)â”œâ”€â”€ scripts/

â”‚   â”œâ”€â”€ ray.sh             # Ray job submission

â”‚   â””â”€â”€ local_ray/         # Ray configuration- [Quick Start](#quick-start)â”‚   â”œâ”€â”€ train.py            # Main training script

â””â”€â”€ README.md

```- [Dependencies](#dependencies)â”‚   â”œâ”€â”€ play.py             # Policy evaluation/playback



## Implementation Status- [Configuration](#configuration)â”‚   â”œâ”€â”€ local_ray/          # Local Ray configuration



### Completed- [Training](#training)â”‚   â”‚   â”œâ”€â”€ job_config.yaml

- G1 robot asset configuration with absolute path resolution

- Actions: 10-DOF arm joint position control- [Results & Monitoring](#results--monitoring)â”‚   â”‚   â””â”€â”€ .env.ray        # W&B credentials

- Observations: joint positions/velocities, base linear/angular velocities

- Rewards: action smoothness penalty (-0.01 weight)- [Known Issues](#known-issues)â”‚   â””â”€â”€ ray/                # Ray cluster utilities

- Termination: episode timeout (30 seconds)

- Training pipeline with video recording- [Contributing](#contributing)â””â”€â”€ README.md

- WANDB integration and logging

- Ray cluster distributed training (2+ GPU nodes)```

- Model checkpointing

## ðŸŽ¯ Overview

### In Progress

- play.py: Policy evaluation script## TODO Checklist

- Factory scene objects and manipulation rewards

- Robot fallen termination conditionThis project provides a complete reinforcement learning framework for training G1 humanoid robots to manipulate objects in factory settings. It leverages:



### Not Started### Phase 1: Asset Setup

- Gripper control

- Curriculum learning- **NVIDIA Isaac Lab**: Physics simulation and rendering- [ ] Load G1 robot URDF/USD file

- Domain randomization

- **PPO Algorithm**: Proximal Policy Optimization from RSL-RL- [ ] Configure factory table/workbench

## Quick Start

- **Ray Cluster**: Distributed training across multiple nodes- [ ] Define manipulable objects (parts)

### Local Training

```bash- **Weights & Biases**: Experiment tracking and visualization- [ ] Set up lighting and camera views

cd scripts

./isaaclab.sh -p train.py --task Isaac-FactoryG1-v0 --num_envs 64 --max_iterations 100- **Gymnasium**: Standard RL environment interface

```

### Phase 2: Environment Configuration

### Ray Cluster Training

```bash### Key Features- [ ] Implement ActionsCfg (arm, gripper, base actions)

cd scripts

./ray.sh job --task Isaac-FactoryG1-v0 --max_iterations 100 --num_envs 64- [ ] Implement ObservationsCfg (joint states, object poses, etc.)

```

- âœ… G1 humanoid robot with 23 DOFs (actuated arm and base locomotion)- [ ] Implement CommandsCfg (target poses/locations)

Monitor on WANDB: https://wandb.ai/jsikka-the-university-of-texas-at-austin/G1_Factory_Test

- âœ… Joint position control for arm manipulation- [ ] Implement RewardsCfg (task-specific rewards)

## Configuration

- âœ… Distributed training with Ray on GPU clusters- [ ] Implement TerminationsCfg (episode termination conditions)

### Training Parameters (agents/ppo_cfg.py)

- Network: 64x64 MLP (actor/critic)- âœ… WANDB integration for experiment tracking

- Learning rate: 1e-3

- PPO clip: 0.2- âœ… Video recording and upload to WANDB### Phase 3: MDP Functions

- Entropy coef: 0.01

- Video logging every 1000 iterations- âœ… Modular MDP configuration system- [ ] Define reward calculation functions



### Environment Parameters (tasks/factory_env_cfg.py)- ðŸ”„ In-progress: Factory scene with objects and manipulation tasks- [ ] Define observation computation functions

- Parallel environments: 64

- Episode length: 30 seconds- [ ] Define action scaling/clipping

- Physics dt: 0.01s

## ðŸ“ Project Structure

## Dependencies

### Phase 4: Training

- Isaac Lab >= 2.1.0

- robot-rl (PPO and distributed training)```- [ ] Implement train.py main training loop

- Ray >= 2.0.0

- WANDB >= 0.15.0g1_factory_isaac/- [ ] Integrate W&B logging

- PyTorch >= 2.0.0

- Gymnasium >= 0.29.0â”œâ”€â”€ assets/- [ ] Set up PPO runner configuration



## Known Issues & Fixesâ”‚   â”œâ”€â”€ g1.urdf                 # G1 humanoid URDF definition- [ ] Test local training



1. Video recording: Fixed args_cli.video flag preservation through initializationâ”‚   â”œâ”€â”€ g1.usd                  # USD variant for visualization

2. Asset paths on Ray: Fixed with os.path.abspath() in g1_cfg.py

3. WANDB credentials: Fixed by sourcing .env.ray in ray_interface.shâ”‚   â”œâ”€â”€ g1_cfg.py               # Asset configuration### Phase 5: Distributed Training (Ray)

4. Robot fallen termination: Removed due to dtype mismatch (base_height_l2 returns float, not bool)

â”‚   â”œâ”€â”€ meshes/                 # Robot mesh files- [ ] Update job_config.yaml with correct paths

## Troubleshooting

â”‚   â””â”€â”€ gi.xml                  # Gripper definition (TODO)- [ ] Add W&B credentials to .env.ray

**GPU Memory Error**: Reduce num_envs or gpu_per_worker

```bashâ”œâ”€â”€ tasks/- [ ] Implement Ray job submission script

./ray.sh job --task Isaac-FactoryG1-v0 --num_envs 32

```â”‚   â”œâ”€â”€ factory_env_cfg.py      # Main environment configuration- [ ] Test on cluster with reduced num_envs



**WANDB Not Logging**: Verify API key in scripts/local_ray/.env.rayâ”‚   â””â”€â”€ __init__.py- [ ] Verify W&B logging on cluster



**Ray Job Fails**: Check cluster statusâ”œâ”€â”€ mdp/

```bash

ray statusâ”‚   â”œâ”€â”€ rewards.py              # Custom reward functions### Phase 6: Policy Evaluation

```

â”‚   â”œâ”€â”€ __init__.py- [ ] Implement play.py for policy evaluation

## Performance

â”œâ”€â”€ agents/- [ ] Add checkpoint loading

- Single GPU: ~3000 steps/second

- 100 iterations: <1 minute on RTX 5090â”‚   â”œâ”€â”€ ppo_cfg.py              # PPO training configuration- [ ] Add visualization

- Training verified on 2-node Ray cluster

â”‚   â””â”€â”€ __init__.py

## Contributing

â”œâ”€â”€ scripts/## Dependencies

1. Add functions to mdp/ for new observations/rewards/actions

2. Update tasks/factory_env_cfg.py to integrate changesâ”‚   â”œâ”€â”€ train.py                # Main training script

3. Test locally before Ray cluster submission

4. Update README with new statusâ”‚   â”œâ”€â”€ play.py                 # Policy evaluation script- Isaac Lab (https://github.com/isaac-sim/IsaacLab)



## Licenseâ”‚   â”œâ”€â”€ ray.sh                  # Ray job submission script- robot_rl (Robot learning framework)



MIT Licenseâ”‚   â”œâ”€â”€ ray_interface.sh        # Ray cluster interface- Ray (Distributed training)



## Acknowledgmentsâ”‚   â”œâ”€â”€ local_ray/- Weights & Biases (Experiment tracking)



- NVIDIA Isaac Lab frameworkâ”‚   â”‚   â”œâ”€â”€ .env.ray            # WANDB credentials

- Unitree G1 URDF

- ETH Zurich RSL-RL implementationâ”‚   â”‚   â””â”€â”€ job_config.yaml     # Ray job configuration## Quick Start

- UT Austin RLGroup guidance

â”‚   â””â”€â”€ ray/

â”‚       â”œâ”€â”€ wrap_resources.py   # Ray resource wrapper### Local Testing

â”‚       â”œâ”€â”€ task_runner.py      # Ray task runner```bash

â”‚       â””â”€â”€ tuner.py            # Ray hyperparameter tunerpython scripts/train.py --task factory-v0 --num_envs 64 --max_iterations 100

â”œâ”€â”€ config/```

â”‚   â””â”€â”€ extension.toml          # IsaacLab extension config

â”œâ”€â”€ pyproject.toml              # Project metadata### Ray Cluster Training

â”œâ”€â”€ setup.py                    # Installation script```bash

â””â”€â”€ README.md# TODO: Implement ray.sh script or use Ray CLI directly

```./ray.sh job --task factory-v0 --num_envs 512 --max_iterations 10000 --wandb

```

## âœ… Implementation Status

## Notes

### Completed âœ“

- G1 humanoid has 12 DOF arm + gripper

#### Phase 1: Asset Setup- Factory scene includes table, parts, target locations

- âœ… G1 robot URDF/USD loaded and configured- Training typically requires 1-2 GPU nodes for stable convergence

- âœ… Asset paths resolved for both local and Ray cluster execution- Use curriculum learning if initial rewards are sparse

- âœ… Articulation and actuator configuration finalized

- âœ… Contact sensors enabled## Resources



#### Phase 2: Environment Configuration- [Isaac Lab Documentation](https://docs.omniverse.nvidia.com/isaacsim/latest/isaac_lab/index.html)

- âœ… **ActionsCfg**: Joint position control for 10 arm/shoulder joints- [Robot RL Framework](https://github.com/leggedrobotics/rsl_rl)

  - Joint names: `.*_shoulder_pitch_joint`, `.*_shoulder_roll_joint`, `.*_shoulder_yaw_joint`, `.*_elbow_joint`, `.*_wrist_roll_joint`- [Ray Tune Documentation](https://docs.ray.io/en/latest/tune/index.html)

  - Action scale: 1.0 (direct joint position commands)
- âœ… **ObservationsCfg**: 52-dimensional observation space
  - Joint positions (relative): `joint_pos`
  - Joint velocities (relative): `joint_vel`
  - Base linear velocity: `base_lin_vel`
  - Base angular velocity: `base_ang_vel`
- âœ… **RewardsCfg**: Action smoothness penalty
  - `action_smoothness`: -0.01 weight on action rate L2 norm
- âœ… **TerminationsCfg**: Episode timeout
  - `time_out`: 30-second episodes (terminates after 1500 steps)

#### Phase 3: MDP Functions
- âœ… All observation functions sourced from isaaclab.envs.mdp
- âœ… Reward functions using action rate L2 norm
- âœ… Proper function names and signatures verified
- âœ… Noise configuration for all observations

#### Phase 4: Training Infrastructure
- âœ… **train.py**: Complete training pipeline
  - Environment instantiation with render_mode support
  - Video recording with gym.wrappers.RecordVideo
  - WANDB integration via robot_rl.runners.OnPolicyRunner
  - Automatic model checkpointing
  - WANDB video upload post-training
- âœ… **ppo_cfg.py**: PPO hyperparameters
  - Network: 64Ã—64 actor/critic MLPs
  - Learning rate: 1e-3
  - Entropy coefficient: 0.01
  - Clip parameter: 0.2
  - Video logging enabled at 1000-iteration intervals
- âœ… Environment variables properly sourced in Ray jobs
- âœ… WANDB credentials configured

#### Phase 5: Distributed Training (Ray)
- âœ… Ray job submission working
- âœ… Multi-GPU node support (tested with 2 GPU nodes, RTX 5090s)
- âœ… 64 parallel environment instances
- âœ… WANDB logging from cluster jobs
- âœ… Model checkpointing to Ray storage
- âœ… Video recording enabled during training

#### Phase 6: Monitoring & Logging
- âœ… WANDB dashboard tracking at: https://wandb.ai/jsikka-the-university-of-texas-at-austin/G1_Factory_Test
- âœ… Metrics logged: episode length, episode return, action smoothness
- âœ… Video frames recorded every 1000 iterations
- âœ… Model checkpoints saved per iteration
- âœ… Training logs with timing information

### In Progress ðŸ”„

- ðŸ”„ **play.py**: Policy evaluation script
  - Currently has TODO placeholders
  - Needs: checkpoint loading, non-headless rendering, policy rollout
  
- ðŸ”„ **Factory Scene Objects**: Manipulation targets
  - Object asset definitions (cubes, parts)
  - Object initialization and randomization
  - Collision detection with robot

- ðŸ”„ **Robot Fallen Termination**: Early episode termination
  - Current: time_out termination only
  - Needed: Custom bool-returning termination for base height check
  - Status: Removed due to dtype mismatch (base_height_l2 is float, not bool)

### Not Started âŒ

- âŒ **Gripper Control**: End-effector manipulation
  - Gripper asset defined but not integrated
  - Needs: gripper action terms, grasp detection

- âŒ **Manipulation Rewards**: Task-specific objectives
  - Object reaching reward
  - Grasping reward
  - Placement/assembly reward

- âŒ **Curriculum Learning**: Progressive task difficulty
  - Initial policies struggle with complex coordination
  - Consider: spawning object at different distances/heights

- âŒ **Advanced RL Techniques**:
  - Asymmetric actor-critic (critic sees full state)
  - Domain randomization
  - Reward shaping with auxiliary losses

## ðŸš€ Quick Start

### Prerequisites

```bash
# Must have Isaac Lab environment set up
# Ensure CUDA-capable GPU is available
# Python 3.11+ with isaaclab and ray installed
```

### Local Training (Single GPU)

```bash
cd scripts
./isaaclab.sh -p train.py --task Isaac-FactoryG1-v0 --num_envs 64 --max_iterations 100
```

### Ray Cluster Training (Distributed)

```bash
cd scripts
# Submit job to Ray cluster with 100 iterations
./ray.sh job --task Isaac-FactoryG1-v0 --max_iterations 100 --num_envs 64

# Monitor in WANDB:
# https://wandb.ai/jsikka-the-university-of-texas-at-austin/G1_Factory_Test
```

### Policy Evaluation

```bash
cd scripts
# TODO: Implement and test play.py
# ./isaaclab.sh -p play.py --task Isaac-FactoryG1-v0 --checkpoint logs/.../model_100.pt
```

## ðŸ“¦ Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| isaaclab | â‰¥2.1.0 | Physics simulation and rendering |
| isaaclab-rl | Latest | Isaac Lab RL integration |
| robot-rl | Latest | RSL-RL PPO and distributed training |
| gymnasium | â‰¥0.29.0 | Standard RL environment API |
| ray | â‰¥2.0.0 | Distributed training framework |
| wandb | â‰¥0.15.0 | Experiment tracking |
| torch | â‰¥2.0.0 | PyTorch for neural networks |
| numpy | Latest | Numerical computations |

### Installation

```bash
# Clone repository
git clone https://github.com/jatinsikka/g1_factory_isaac.git
cd g1_factory_isaac

# Install in development mode
pip install -e .

# Or manually install dependencies
pip install -e .[all]
```

## âš™ï¸ Configuration

### Training Parameters (`agents/ppo_cfg.py`)

```python
VanillaPPORunnerCfg:
  num_steps_per_env: 24         # Steps per environment per iteration
  max_iterations: 10_100         # Total training iterations
  save_interval: 1_000           # Save checkpoint every 1000 iterations
  learning_rate: 1.0e-3          # Adam optimizer learning rate
  clip_param: 0.2                # PPO clipping parameter
  entropy_coef: 0.01             # Entropy regularization
  gamma: 0.99                    # Discount factor
  lam: 0.95                      # GAE lambda
  log_video: True                # Enable video recording
  video_interval: 1_000          # Record video every 1000 iterations
  video_length: 200              # Frames per video
```

### Environment Parameters (`tasks/factory_env_cfg.py`)

```python
SceneCfg:
  num_envs: 256                 # Number of parallel environments
  env_spacing: 2.5              # Distance between environment copies
  episode_length_s: 30.0        # Episode duration in seconds
  decimation: 2                 # Simulation step ratio (1/decimation)
  sim.dt: 0.01                  # Physics timestep
```

### WANDB Credentials (`.env.ray`)

```bash
WANDB_API_KEY=<your-api-key>
WANDB_USERNAME=jsikka
```

## ðŸ“Š Training

### Single GPU Training
- **GPU**: NVIDIA RTX 5090
- **Num Environments**: 64
- **Steps/Second**: ~3000
- **Time per Iteration**: ~0.5 seconds
- **Max Iterations**: 100 (typically completes in <1 minute)

### Multi-GPU Cluster Training
- **Cluster**: Ray with 2 GPU nodes
- **Total GPUs**: 2Ã—RTX 5090
- **Num Environments**: 64 per job
- **Architecture**: 64Ã—64 MLP (actor & critic)
- **Training Status**: âœ… Tested and working

### Training Metrics

The training loop tracks:
- **Episode Length**: Mean timesteps per episode
- **Episode Return**: Cumulative reward per episode
- **Action Smoothness**: L2 norm of action changes
- **Learning Rate**: Current LR for Adam optimizer
- **Policy Loss**: PPO actor loss
- **Value Loss**: Critic regression loss

## ðŸ“ˆ Results & Monitoring

### WANDB Dashboard

All training runs are logged to WANDB for visualization and analysis:

**Project**: https://wandb.ai/jsikka-the-university-of-texas-at-austin/G1_Factory_Test

**Logged Metrics**:
- Training curves for all losses
- Episode metrics (length, return)
- Video recordings from policy rollouts
- Hyperparameter values
- System resource usage (GPU memory, etc.)

### Checkpoint Management

Checkpoints are saved to:
```
logs/g1_factory_test/{timestamp}_{run_name}/
â”œâ”€â”€ model_0.pt          # Initial model
â”œâ”€â”€ model_100.pt        # Model at iteration 100
â”œâ”€â”€ params/
â”‚   â”œâ”€â”€ env.yaml        # Environment config snapshot
â”‚   â””â”€â”€ agent.yaml      # Agent config snapshot
â””â”€â”€ videos/train/       # Recorded rollout videos
    â”œâ”€â”€ rl-video-episode-0.mp4
    â””â”€â”€ rl-video-episode-100.mp4
```

## ðŸ› Known Issues

### Video Recording Not Working
- **Symptom**: No videos in WANDB dashboard after training
- **Cause**: `args_cli.video` flag being set to False before wrapper initialization
- **Status**: âœ… **FIXED** - Flag now properly preserved for video wrapper
- **Solution**: Updated train.py to keep video flag through initialization

### Asset Path Resolution on Ray
- **Symptom**: `RuntimeError: Failed to find articulation when resolving '/World/envs/env_0/robot'`
- **Cause**: Relative paths not resolving on Ray worker nodes
- **Status**: âœ… **FIXED** - Updated g1_cfg.py to use absolute paths with `os.path.abspath()`
- **Solution**: Changed from `Path(__file__).parent / "g1.urdf"` to `os.path.join(ASSETS_DIR, "g1.urdf")`

### WANDB Environment Variables Not Sourced
- **Symptom**: WANDB credentials not available on Ray jobs
- **Cause**: `.env.ray` not sourced before job submission
- **Status**: âœ… **FIXED** - Added `source $SCRIPT_DIR/.env.ray` in ray_interface.sh
- **Solution**: Modified ray_interface.sh to source credentials before python command

### Robot Fallen Termination Dtype Mismatch
- **Symptom**: `Expected Bool tensor, got Float tensor` error
- **Cause**: `mdp_isaac.base_height_l2` returns float, not bool
- **Status**: âœ… **WORKAROUND** - Removed termination, using time_out only
- **Solution**: Custom termination function needed (marked as TODO)

### No Visualization in Headless Mode
- **Symptom**: No GUI window during training
- **Cause**: Ray cluster runs headless for efficiency
- **Status**: âœ… **EXPECTED** - Designed behavior for cluster training
- **Solution**: Use `play.py` for local visualization with trained checkpoints

## ðŸ”§ Troubleshooting

### Training Fails with GPU Memory Error
```bash
# Reduce parallel environments
./ray.sh job --task Isaac-FactoryG1-v0 --num_envs 32 --max_iterations 100

# Or request lower GPU allocation
./ray.sh job --task Isaac-FactoryG1-v0 --gpu_per_worker 0.5
```

### WANDB Not Logging
```bash
# Verify API key in .env.ray
cat scripts/local_ray/.env.ray

# Check WANDB initialization in logs
grep -i "wandb" logs/g1_factory_test/*/training_output.log
```

### Ray Job Submission Fails
```bash
# Check Ray cluster status
ray status

# Verify cluster has GPUs
ray cluster compute-resource-utilization
```

## ðŸ“ Contributing

When adding new features:

1. **Add to `mdp/`** for observation/reward/action functions
2. **Update `tasks/factory_env_cfg.py`** to integrate into environment
3. **Test locally** first: `./isaaclab.sh -p train.py ...`
4. **Run on Ray** to verify cluster compatibility
5. **Update this README** with new status

## ðŸ“„ License

MIT License - See LICENSE file for details

## ðŸ™ Acknowledgments

- **NVIDIA**: Isaac Lab framework
- **Unitree**: G1 robot URDF
- **ETH Zurich**: RSL-RL PPO implementation
- **UT Austin RLGroup**: Project context and guidance

## ðŸ“ž Support & Questions

For issues or questions:
1. Check [Known Issues](#known-issues) section
2. Review WANDB logs for training diagnostics
3. Check Ray cluster status: `ray status`
4. Review Isaac Lab documentation for environment-specific issues

---

**Last Updated**: November 22, 2025  
**Status**: Training pipeline functional and tested on Ray cluster âœ…
